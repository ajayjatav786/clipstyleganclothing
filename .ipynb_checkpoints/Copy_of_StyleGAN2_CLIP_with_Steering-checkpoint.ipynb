{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhD1FafIaZiH",
    "outputId": "c6813c57-f31d-400b-d9a5-cebff2fc5283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CLIP_Steering'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 25 (delta 8), reused 20 (delta 3), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (25/25), done.\n",
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/fengqingthu/CLIP_Steering.git\n",
    "%cd CLIP_Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV0COHSPb_FH"
   },
   "source": [
    "Install clip + stylegan2 dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKEybkD400r2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YWp1gBBi8tM",
    "outputId": "c071f775-875c-41ab-a06b-01442e5a3230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /home/aj/anaconda3/lib/python3.7/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /home/aj/anaconda3/lib/python3.7/site-packages (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/aj/anaconda3/lib/python3.7/site-packages (4.59.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/aj/anaconda3/lib/python3.7/site-packages (from ftfy) (0.2.6)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xd8r278c\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xd8r278c\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/aj/anaconda3/lib/python3.7/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /home/aj/anaconda3/lib/python3.7/site-packages (from clip==1.0) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/aj/anaconda3/lib/python3.7/site-packages (from clip==1.0) (4.59.0)\n",
      "Requirement already satisfied: torch in /home/aj/anaconda3/lib/python3.7/site-packages (from clip==1.0) (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /home/aj/anaconda3/lib/python3.7/site-packages (from clip==1.0) (0.14.1+cu116)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/aj/anaconda3/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: typing-extensions in /home/aj/anaconda3/lib/python3.7/site-packages (from torch->clip==1.0) (4.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/aj/anaconda3/lib/python3.7/site-packages (from torchvision->clip==1.0) (9.0.1)\n",
      "Requirement already satisfied: requests in /home/aj/anaconda3/lib/python3.7/site-packages (from torchvision->clip==1.0) (2.23.0)\n",
      "Requirement already satisfied: numpy in /home/aj/anaconda3/lib/python3.7/site-packages (from torchvision->clip==1.0) (1.21.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/aj/anaconda3/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aj/anaconda3/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2019.3.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/aj/anaconda3/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/aj/anaconda3/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2.8)\n"
     ]
    }
   ],
   "source": [
    "# !conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZBnAjACbhKm",
    "outputId": "ea0c63a2-e6dd-409d-df07-ba9ec3e0d171"
   },
   "outputs": [],
   "source": [
    "# !pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st3m7E1AlIj-"
   },
   "source": [
    "Clone the CLIP repo and test that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGT4D-FVkICz",
    "outputId": "c663cdc1-d728-48e5-f0e5-29edf54d6d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CLIP'...\n",
      "remote: Enumerating objects: 243, done.\u001b[K\n",
      "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
      "Receiving objects: 100% (243/243), 8.92 MiB | 2.93 MiB/s, done.\n",
      "Resolving deltas: 100% (124/124), done.\n",
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/CLIP\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/CLIP.git\n",
    "%cd CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qHzPE4BjJpW",
    "outputId": "fd509077-5bd1-4d78-a08c-71759c69f83c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9927   0.004253 0.003016]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mheX4PDHkSbS",
    "outputId": "ce9fc781-492a-416d-b782-c909069ea958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering\n"
     ]
    }
   ],
   "source": [
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/\n",
    "# %cd /content/CLIP_Steering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uvy7C3PnoLj"
   },
   "source": [
    "Clone the styleGAN2 repo and test that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vi3jpP30nr9J",
    "outputId": "e083d667-cbdd-4d58-fa0b-125e5896e527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan2-ada-pytorch'...\n",
      "remote: Enumerating objects: 128, done.\u001b[K\n",
      "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
      "Receiving objects: 100% (128/128), 1.12 MiB | 4.20 MiB/s, done.\n",
      "Resolving deltas: 100% (57/57), done.\n",
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "# %cd /content/CLIP_Steering/stylegan2-ada-pytorch/\n",
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvc5d1TylD9B"
   },
   "source": [
    "Download pretrained styleGAN2 model. A couple of more options on: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KeJjyATqcDgo",
    "outputId": "c37da3a2-3af3-4f4e-8ca5-c03d23dc999f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained’: File exists\n",
      "--2023-04-10 14:03:52--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
      "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 18.164.202.26, 18.164.202.46, 18.164.202.62, ...\n",
      "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|18.164.202.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 381624121 (364M) [binary/octet-stream]\n",
      "Saving to: ‘/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained/ffhq.pkl’\n",
      "\n",
      "/media/aj/9c4728aa- 100%[===================>] 363.94M  4.83MB/s    in 80s     \n",
      "\n",
      "2023-04-10 14:05:14 (4.53 MB/s) - ‘/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained/ffhq.pkl’ saved [381624121/381624121]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir /content/CLIP_Steering/pretrained & wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O /content/CLIP_Steering/pretrained/ffhq.pkl\n",
    "    \n",
    "    \n",
    "!mkdir /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained & wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained/ffhq.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdJe0MnpvKVX",
    "outputId": "2ec4b040-9acc-4c12-8d3b-9a912d1806d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "11JojyKW0eg2"
   },
   "outputs": [],
   "source": [
    "!cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbZoQusFvjgX",
    "outputId": "75a6c1ef-838c-4cfa-ab86-4d6fbf9ca826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GANalyze'...\n",
      "remote: Enumerating objects: 57, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 57 (delta 1), reused 4 (delta 1), pack-reused 50\u001b[K\n",
      "Unpacking objects: 100% (57/57), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LoreGoetschalckx/GANalyze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2w85Cueyv6Sr",
    "outputId": "1531faca-54fb-4d18-ba23-876594f64bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch/GANalyze\n"
     ]
    }
   ],
   "source": [
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch/GANalyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZG9PNXeki8t",
    "outputId": "aaea299c-42cb-43b4-f973-e942ac22c7ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"../pretrained/afhqcat.pkl\"...\n",
      "Generating image for seed 76 (0/4) ...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "Generating image for seed 165 (1/4) ...\n",
      "Generating image for seed 197 (2/4) ...\n",
      "Generating image for seed 749 (3/4) ...\n"
     ]
    }
   ],
   "source": [
    "#Generate face images from pretrained stylegan on faces.\n",
    "#change  seed numbers to get different out images if you like. Try Metfaces for diff variety of face images\n",
    "#check out folder to verify that generated images have appeared\n",
    "!python generate.py --outdir=out --trunc=1 --seeds=76,165,197,749 \\\n",
    "    --network=../pretrained/afhqcat.pkl\n",
    "#    --network=../pretrained/imagenet.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7NGWKTst3F-",
    "outputId": "0c6d507c-92ab-4ab9-d9c3-8f263eec13eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\n",
    "\n",
    "# %cd /content/CLIP_Steering/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sf4x5BWnpsEq",
    "outputId": "7cbaf88e-c498-4fb5-cd04-9bf5edb18c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering\n"
     ]
    }
   ],
   "source": [
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PU5MPBazqNKq",
    "outputId": "db5287de-2fe3-4f0f-d680-ea71b469d7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering\r\n"
     ]
    }
   ],
   "source": [
    "!pwd # Make sure the current working directory is /content/CLIP_Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRWE5T3ev2sl"
   },
   "source": [
    "An easy fix to make the dnnlib and torch_utils modules within the styleGAN2 repo accessible from PYTHONPATH - simply copy them to the project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2OrOhPgLtqri"
   },
   "outputs": [],
   "source": [
    "!cp -a stylegan2-ada-pytorch/dnnlib dnnlib\n",
    "!cp -a stylegan2-ada-pytorch/torch_utils torch_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vCVeZHYx3CM"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYy5g5GDxM4R",
    "outputId": "e5c2da5b-beaa-41a9-d0c3-87256489e635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch/\n",
    "!cp -a /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwedjqWVx_VV",
    "outputId": "0e589528-60cc-4817-c5a7-edef1965f6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcUGd3kPwDje"
   },
   "source": [
    "The following code is from the file ganalyze_with_clip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-pQ2AUXxDFyI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'MAX_SPLIT_SIZE_1=32MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0759stIpDH9R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeZI__HnyXf1",
    "outputId": "d4635c09-40ba-41fd-d2f0-f20ae99e6dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/stylegan2-ada-pytorch\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5Bj35RQzDN2",
    "outputId": "4e7f8329-ef67-4009-9274-5568d4936cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/CLIP_Steering/\n",
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "P3Pm6qcOespK"
   },
   "outputs": [],
   "source": [
    "# Here is how to change the GAN model\n",
    "# gan_model_path = '/content/CLIP_Steering/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "\n",
    "gan_model_path = '/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/CLIP_Steering/pretrained/ffhq.pkl'\n",
    "# Here is how we specify the desired attributes\n",
    "attributes = [\"a cartoon cat \", \"a cat\"]\n",
    "#attributes = [\"an old face\", \"a young face\", \"a happy face\",\"a sad face\"]\n",
    "class_index = 0 # which attribute do we want to maximize, ie. attribute number 0, or attribute number 1 etc.\n",
    "\n",
    "# Here is where to store the checkpoints i.e. weights \n",
    "checkpoint_dir = f'checkpoints/results_maximize_{attributes[class_index]}_probability'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGv4MoraqzW5",
    "outputId": "8ca260cf-9ebc-4b3a-c1b1-36a4c4db5d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0180,  0.0051, -0.0376,  ..., -0.0305, -0.0396,  0.0076],\n",
       "        [ 0.0294,  0.0186, -0.0361,  ..., -0.0414, -0.0374,  0.0042]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as torch_transforms\n",
    "import pickle\n",
    "import os\n",
    "import pathlib\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "# sys.path.append('/content/CLIP_Steering/stylegan2-ada-pytorch/')\n",
    "\n",
    "sys.path.append('/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/CLIP_Steering/stylegan2-ada-pytorch/')\n",
    "\n",
    "import torch_utils\n",
    "import ganalyze_transformations as transformations\n",
    "import ganalyze_common_utils as common\n",
    "from clip_classifier_utils import SimpleTokenizer\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def plot_save_image(img, out_dir):\n",
    "    img = (gan_images.permute(0, 2, 3, 1)).clamp(0, 255).to(torch.uint8)\n",
    "    img_np = img.detach().cpu().numpy().squeeze()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 6))\n",
    "    for i in range(6): # By defualt batchsize= 6\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        ax[row, col].imshow(img_np[i])\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(6):\n",
    "      filename = f\"image_{batch_start}_{i}.png\"\n",
    "      plt.imsave(f\"{checkpoint_dir}/{filename}\", img_np[i])\n",
    "\n",
    "def gan_output_transform(imgs):\n",
    "    # Input:\n",
    "    # img: NCHW\n",
    "    #\n",
    "    # Output\n",
    "    # img_np: HWC RGB image\n",
    "\n",
    "    imgs = (imgs * 127.5 + 128).clamp(0, 255).float()\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def clip_input_transform(images):\n",
    "    # Input\n",
    "    # img_np: torch tensor of shape NHWC, RGB image\n",
    "    #\n",
    "    # Output\n",
    "    # image_input: torch tensor of shape NHWC\n",
    "\n",
    "    image_mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "    image_std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "    transform = torch.nn.Sequential(\n",
    "        torch_transforms.Resize((256, 256)),\n",
    "        torch_transforms.CenterCrop((224, 224)),\n",
    "        torch_transforms.Normalize(image_mean, image_std),\n",
    "    )\n",
    "\n",
    "    return transform(images)\n",
    "\n",
    "def get_clip_scores(image_inputs, encoded_text, model, class_index=0):\n",
    "    #TODO: clarify class index\n",
    "    image_inputs = clip_input_transform(image_inputs).to(device)\n",
    "    image_feats = model.encode_image(image_inputs).float()\n",
    "    image_feats = F.normalize(image_feats, p=2, dim=-1)\n",
    "\n",
    "    similarity_scores = torch.matmul(image_feats, torch.transpose(encoded_text, 0, 1))\n",
    "    similarity_scores = similarity_scores.to(device)\n",
    "    return similarity_scores.narrow(dim=-1, start=class_index, length=1).squeeze(dim=-1)\n",
    "\n",
    "def get_clip_probs(image_inputs, encoded_text, model, class_index=0):\n",
    "    image_inputs = clip_input_transform(image_inputs).to(device)\n",
    "    image_feats = model.encode_image(image_inputs).float()\n",
    "    image_feats = F.normalize(image_feats, p=2, dim=-1)\n",
    "\n",
    "    clip_probs = (100.0 * torch.matmul(image_feats, torch.transpose(encoded_text, 0, 1))).softmax(dim=-1)\n",
    "    clip_probs = clip_probs.to(device)\n",
    "\n",
    "    return clip_probs.narrow(dim=-1, start=class_index, length=1).squeeze(dim=-1)\n",
    "\n",
    "# Set up GAN\n",
    "# Initialize GAN generator and transforms\n",
    "with open(gan_model_path, 'rb') as f:\n",
    "    G = pickle.load(f)['G_ema']\n",
    "G.eval()\n",
    "G.to(device)\n",
    "latent_space_dim = G.z_dim\n",
    "\n",
    "# Set up clip classifier\n",
    "clip_model_path = '/home/aj/.cache/clip/ViT-B-32.pt'\n",
    "clip_model = torch.jit.load(clip_model_path)\n",
    "clip_model.eval()\n",
    "clip_model.to(device)\n",
    "input_resolution = clip_model.input_resolution.item()\n",
    "context_length = clip_model.context_length.item()\n",
    "vocab_size = clip_model.vocab_size.item()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Extract text features for clip\n",
    "\n",
    "# Here is how we specify the desired attributes\n",
    "# attributes = [\"an evil face\", \"an innocent face\"]\n",
    "# class_index = 0 # which attribute do we want to maximize\n",
    "\n",
    "tokenizer = SimpleTokenizer(\"CLIP/clip/bpe_simple_vocab_16e6.txt.gz\")\n",
    "sot_token = tokenizer.encoder['<|startoftext|>']\n",
    "eot_token = tokenizer.encoder['<|endoftext|>']\n",
    "text_descriptions = [f\"This is a photo of {label}\" for label in attributes]\n",
    "text_tokens = [[sot_token] + tokenizer.encode(desc) + [eot_token] for desc in text_descriptions]\n",
    "text_inputs = torch.zeros(len(text_tokens), clip_model.context_length, dtype=torch.long)\n",
    "\n",
    "for i, tokens in enumerate(text_tokens):\n",
    "    text_inputs[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "# These are held constant through the optimization, akin to labels\n",
    "text_inputs = text_inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_inputs).float()\n",
    "    text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "text_features.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66HoImoYq2RG",
    "outputId": "a8939af5-3661-4877-bf2a-455d316779da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "approach:  one_direction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up Transformer, i.e. the function that transforms the input z vector \n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "transformer_params = ['OneDirection', 'None']\n",
    "transformer = transformer_params[0]\n",
    "transformer_arguments = transformer_params[1]\n",
    "if transformer_arguments != \"None\":\n",
    "    key_value_pairs = transformer_arguments.split(\",\")\n",
    "    key_value_pairs = [pair.split(\"=\") for pair in key_value_pairs]\n",
    "    transformer_arguments = {pair[0]: pair[1] for pair in key_value_pairs}\n",
    "else:\n",
    "    transformer_arguments = {}\n",
    "\n",
    "transformation = getattr(transformations, transformer)(latent_space_dim, vocab_size, **transformer_arguments)\n",
    "transformation = transformation.to(device)\n",
    "\n",
    "# function that is used to score the (attribute, image) pair\n",
    "scoring_fun = get_clip_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "pyUD6MXSrwJF"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(transformation.parameters(), lr=0.0002) #as specified in GANalyze\n",
    "losses = common.AverageMeter(name='Loss')\n",
    "\n",
    "#  training settings\n",
    "optim_iter = 0\n",
    "batch_size = 1 # Do not change\n",
    "train_alpha_a = -0.9 # Lower limit for step sizes\n",
    "train_alpha_b = 0.9 # Upper limit for step sizes\n",
    "num_samples = 450 # Number of samples to train for # Ganalyze uses 400,000 samples. Use smaller number for testing.\n",
    "\n",
    "# create training set\n",
    "#np.random.seed(seed=0)\n",
    "truncation = 1\n",
    "zs = common.truncated_z_sample(num_samples, latent_space_dim, truncation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h81oZH3MBRLN",
    "outputId": "008e7309-be14-4881-b655-a30fa67d1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/CLIP_Steering/ClothingGAN/\n",
    "%cd /media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "g-M1AGgNBbwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache\t\t\t\t\t    lookbook-interpolation.gif\r\n",
      "CLIP_Steering\t\t\t\t    models\r\n",
      "ClothingGAN_Demo-Copy1.ipynb\t\t    netdissect\r\n",
      "ClothingGAN_Demo.ipynb\t\t\t    new_i.png\r\n",
      "clothing-gan.gif\t\t\t    notebooks\r\n",
      "clothing-gan-thumbnail.gif\t\t    out\r\n",
      "config.py\t\t\t\t    __pycache__\r\n",
      "Copy_of_StyleGAN2_CLIP_with_Steering.ipynb  README.md\r\n",
      "decomposition.py\t\t\t    SETUP.md\r\n",
      "deps\t\t\t\t\t    teaser.jpg\r\n",
      "environment.yml\t\t\t\t    tests\r\n",
      "estimators.py\t\t\t\t    TkTorchWindow.py\r\n",
      "interactive.py\t\t\t\t    utils.py\r\n",
      "LICENSE\t\t\t\t\t    visualize.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "Ac8WACZBBLDt",
    "outputId": "cbd88c4f-5f03-4c14-9801-8838e18d9ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StyleGAN2: Optimized CUDA op FusedLeakyReLU not available, using native PyTorch fallback.\n",
      "StyleGAN2: Optimized CUDA op UpFirDn2d not available, using native PyTorch fallback.\n"
     ]
    }
   ],
   "source": [
    "#@title Load Model\n",
    "selected_model = 'lookbook'\n",
    "\n",
    "# Load model\n",
    "from IPython.utils import io\n",
    "import torch\n",
    "import PIL\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "import imageio\n",
    "# from ClothingGAN.models import get_instrumented_model\n",
    "from models import get_instrumented_model\n",
    "\n",
    "from decomposition import get_or_compute\n",
    "from config import Config\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Speed up computation\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Specify model to use\n",
    "config = Config(\n",
    "  model='StyleGAN2',\n",
    "  layer='style',\n",
    "  output_class=selected_model,\n",
    "  components=80,\n",
    "  use_w=True,\n",
    "  batch_size=5_000, # style layer quite small\n",
    ")\n",
    "\n",
    "inst = get_instrumented_model(config.model, config.output_class,\n",
    "                              config.layer, torch.device('cuda'), use_w=config.use_w)\n",
    "model = inst.model\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "w_l = model.sample_latent(1, seed=5).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 512)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = [w_l]*model.get_max_latents() # one per layer\n",
    "np.array(w1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 512)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-1a856a6a94aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/models/wrappers.py\u001b[0m in \u001b[0;36msample_np\u001b[0;34m(self, z, n_samples, seed)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mimg_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/models/wrappers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         out, _ = self.model(x, noise=self.noise,\n\u001b[0;32m--> 193\u001b[0;31m             truncation=self.truncation, truncation_latent=self.latent_avg, input_is_w=self.w_primary)\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/models/stylegan2/stylegan2-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, styles, return_latents, inject_index, truncation, truncation_latent, input_is_w, noise, randomize_noise)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgb1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/models/stylegan2/stylegan2-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, style, noise)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# out = out + self.bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/models/stylegan2/stylegan2-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, style)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/aj/9c4728aa-3a45-44cf-ada0-079baa4684ac/home/webtunix/stylegan/ClothingGAN/models/stylegan2/stylegan2-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             out = F.linear(\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_mul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             )\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "im2 = model.sample_np(zs)\n",
    "plt.imshow(com)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "_8IYdB1FBHs1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "slice(0, 1, None)\n",
      "<class 'slice'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c9ed62ee1e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mgan_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_output_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     out_scores = scoring_fun(\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mimage_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# TODO: ignore z vectors with less confident clip scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5e77a45acb0e>\u001b[0m in \u001b[0;36mget_clip_probs\u001b[0;34m(image_inputs, encoded_text, model, class_index)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_clip_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mimage_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_input_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mimage_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mimage_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5e77a45acb0e>\u001b[0m in \u001b[0;36mclip_input_transform\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_clip_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# checkpoint_dir = f'checkpoints/results_maximize_{attributes[class_index]}_probability'\n",
    "pathlib.Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# loop over data batches\n",
    "for batch_start in range(0, num_samples, batch_size):\n",
    "\n",
    "    # input batch\n",
    "    s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "    z = torch.from_numpy(zs[s]).type(torch.FloatTensor).to(device)\n",
    "    print(type(z))\n",
    "    print(s)\n",
    "    print(type(s))\n",
    "    y = None\n",
    "    \n",
    "    #step_sizes = (train_alpha_b - train_alpha_a)*np.ones(batch_size)*0.0001\n",
    "    #print(step_sizes)\n",
    "\n",
    "    step_sizes = (train_alpha_b - train_alpha_a) * \\\n",
    "        np.random.random(size=(batch_size)) + train_alpha_a  # sample step_sizes\n",
    "    \n",
    "    step_sizes_broadcast = np.repeat(step_sizes, latent_space_dim).reshape([batch_size, latent_space_dim])\n",
    "    step_sizes_broadcast = torch.from_numpy(step_sizes_broadcast).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    # ganalyze steps\n",
    "#     gan_images = G(z, None)\n",
    "    gan_images = model.sample_np(z)\n",
    "    gan_images=torch.from_numpy(gan_images).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    gan_images = gan_output_transform(gan_images)\n",
    "    out_scores = scoring_fun(\n",
    "        image_inputs=gan_images, encoded_text=text_features, model=clip_model, class_index=class_index,\n",
    "    )\n",
    "    # TODO: ignore z vectors with less confident clip scores\n",
    "    target_scores = out_scores + torch.from_numpy(step_sizes).to(device).float()\n",
    "\n",
    "    z_transformed = transformation.transform(z, None, step_sizes_broadcast)\n",
    "#     gan_images_transformed = G(z_transformed, None)\n",
    "    gan_images_transformed = model(z_transformed)\n",
    "    gan_images_transformed=torch.from_numpy(gan_images_transformed).type(torch.FloatTensor).to(device)\n",
    "\n",
    "\n",
    "    gan_images_transformed = gan_output_transform(gan_images_transformed).to(device)\n",
    "    out_scores_transformed = scoring_fun(\n",
    "        image_inputs=gan_images_transformed, encoded_text=text_features, model=clip_model, class_index=class_index,\n",
    "    ).to(device).float()\n",
    "\n",
    "    # compute loss\n",
    "    loss = transformation.criterion(out_scores_transformed, target_scores)\n",
    "\n",
    "    # backwards\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print loss\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    if optim_iter % 10 == 0:\n",
    "        logger.info(f'[Maximizing score for {attributes[class_index]}] Progress: [{batch_start}/{num_samples}] {losses}')\n",
    "        print(f'[Maximizing score for {attributes[class_index]}] Progress: [{batch_start}/{num_samples}] {losses}')\n",
    "        \n",
    "\n",
    "    if optim_iter % 50 == 0:\n",
    "        logger.info(f\"saving checkpoint at iteration {optim_iter}\")\n",
    "        print(f\"saving checkpoint at iteration {optim_iter}\")\n",
    "        torch.save(transformation.state_dict(), os.path.join(checkpoint_dir, \"pytorch_model_{}.pth\".format(batch_start)))\n",
    "\n",
    "        # plot and save sample images\n",
    "        plot_save_image(gan_images, checkpoint_dir)\n",
    "\n",
    "    optim_iter = optim_iter + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkYR11tbMw6r"
   },
   "source": [
    "Now that the model is trained, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF9gspXC622O"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "import torch_utils\n",
    "import ganalyze_transformations as transformations\n",
    "import ganalyze_common_utils as common\n",
    "from clip_classifier_utils import SimpleTokenizer\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "#TODO Figure out where to resume checkpoints\n",
    "\n",
    "def one_hot(index, vocab_size=1000):\n",
    "    output = torch.zeros(index.size(0), vocab_size).to(index.device)\n",
    "    output.scatter_(1, index.unsqueeze(-1), 1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1) * 255\n",
    "\n",
    "# helper function for visualization of test images\n",
    "#def make_image(z, y, step_size, transform):\n",
    "def make_image(z, step_size, transform):\n",
    "\n",
    "    if transform:\n",
    "        z_transformed = transformation.transform(z,None, step_size)\n",
    "        z_transformed = z.norm() * z_transformed / z_transformed.norm()\n",
    "        z = z_transformed\n",
    "\n",
    "    #gan_images = utils.pytorch.denorm(generator(z, y))\n",
    "    #gan_images = common.pytorch.denorm(G(z, y))\n",
    "    #gan_images = G(z, None)\n",
    "    gan_images = denorm(G(z, y))\n",
    "    gan_images_np = gan_images.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "    gan_images = gan_output_transform(gan_images)\n",
    "\n",
    "    #gan_images = gan_images.view(-1, *gan_images.shape[-3:])\n",
    "    gan_images = gan_images.to(device)\n",
    "\n",
    "    #out_scores_current = output_transform(assessor(gan_images))\n",
    "    #out_scores_current = gan_output_transform(scoring_fun(gan_images))\n",
    "    #TODO Check assessor section in Ganalyze test script line 106\n",
    "    out_scores_current = scoring_fun(image_inputs=gan_images, encoded_text=text_features, model=clip_model, class_index=class_index).to(device).float()\n",
    "    out_scores_current = out_scores_current.detach().cpu().numpy()\n",
    "    if len(out_scores_current.shape) == 1:\n",
    "        out_scores_current = np.expand_dims(out_scores_current, 1)\n",
    "\n",
    "    return(gan_images_np, z, out_scores_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBALFuVwC8PH",
    "outputId": "7694a903-adfa-4785-e2eb-8c13a1eff97b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterative\n",
      "iterative\n",
      "iterative\n",
      "y:  0\n"
     ]
    }
   ],
   "source": [
    "# Test settings\n",
    "\n",
    "#code from ganalyze_commons_utils added below \n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import PIL.ImageDraw\n",
    "import PIL.ImageFont\n",
    "\n",
    "\n",
    "def truncated_z_sample(batch_size, dim_z, truncation=1):\n",
    "    values = truncnorm.rvs(-2, 2, size=(batch_size, dim_z))\n",
    "    return truncation * values\n",
    "\n",
    "\n",
    "def imgrid(imarray, cols=5, pad=1):\n",
    "    if imarray.dtype != np.uint8:\n",
    "        imarray = np.uint8(imarray)\n",
    "        # raise ValueError('imgrid input imarray must be uint8')\n",
    "    pad = int(pad)\n",
    "    assert pad >= 0\n",
    "    cols = int(cols)\n",
    "    assert cols >= 1\n",
    "    N, H, W, C = imarray.shape\n",
    "    rows = int(np.ceil(N / float(cols)))\n",
    "    batch_pad = rows * cols - N\n",
    "    assert batch_pad >= 0\n",
    "    post_pad = [batch_pad, pad, pad, 0]\n",
    "    pad_arg = [[0, p] for p in post_pad]\n",
    "    imarray = np.pad(imarray, pad_arg, 'constant', constant_values=255)\n",
    "    H += pad\n",
    "    W += pad\n",
    "    grid = (imarray\n",
    "            .reshape(rows, cols, H, W, C)\n",
    "            .transpose(0, 2, 1, 3, 4)\n",
    "            .reshape(rows * H, cols * W, C))\n",
    "    if pad:\n",
    "        grid = grid[:-pad, :-pad]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def annotate_outscore(array, outscore):\n",
    "    for i in range(array.shape[0]):\n",
    "        I = PIL.Image.fromarray(np.uint8(array[i, :, :, :]))\n",
    "        draw = PIL.ImageDraw.Draw(I)\n",
    "        #font = PIL.ImageFont.truetype(\"arial.ttf\", int(array.shape[1]/8.5))\n",
    "        font = PIL.ImageFont.load_default()\n",
    "        message = str(round(np.squeeze(outscore)[i], 2))\n",
    "        x, y = (0, 0)\n",
    "        w, h = font.getsize(message)\n",
    "        #print(w, h)\n",
    "        draw.rectangle((x, y, x + w, y + h), fill='white')\n",
    "        draw.text((x, y), message, fill=\"black\", font=font)\n",
    "        array[i, :, :, :] = np.array(I)\n",
    "    return(array)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "#Test settings\n",
    "\n",
    "num_samples = 10\n",
    "truncation = 1\n",
    "iters = 3\n",
    "#np.random.seed(seed=999) #removed like training code block\n",
    "annotate = True\n",
    "vocab_size = 1 #vocab size is one for debugging. else change to:\n",
    "#vocab_size = clip_model.vocab_size.item()\n",
    "\n",
    "if vocab_size == 0:\n",
    "    num_categories = 1\n",
    "else:\n",
    "    num_categories = vocab_size\n",
    "\n",
    "for y in range(num_categories):\n",
    "\n",
    "    ims = []\n",
    "    outscores = []\n",
    "\n",
    "    #zs = utils.common.truncated_z_sample(num_samples, dim_z, truncation)        \n",
    "    #zs = common.truncated_z_sample(num_samples, latent_space_dim, truncation)\n",
    "    zs = truncated_z_sample(num_samples, latent_space_dim, truncation)\n",
    "\n",
    "    ys = np.repeat(y, num_samples)\n",
    "    zs = torch.from_numpy(zs).type(torch.FloatTensor).to(device)\n",
    "    ys = torch.from_numpy(ys).to(device)\n",
    "    ys = one_hot(ys, vocab_size)\n",
    "    #step_sizes = np.repeat(np.array(opts[\"alpha\"]), num_samples * dim_z).reshape([num_samples, dim_z])\n",
    "\n",
    "    alpha = 0.2\n",
    "    step_sizes = np.repeat((alpha), num_samples * latent_space_dim).reshape([num_samples, latent_space_dim])\n",
    "\n",
    "    # TODO instead write a loop here to sample values of alpha\n",
    "    #alpha= [-0.5,-0.25,0,0.25,0.5]\n",
    "    #step_sizes = np.repeat(np.array[of alpha], num_samples * latent_space_dim).reshape([num_samples, latent_space_dim])\n",
    "\n",
    "    step_sizes = torch.from_numpy(step_sizes).type(torch.FloatTensor).to(device)\n",
    "    feed_dicts = []\n",
    "    for batch_start in range(0, num_samples, 4):\n",
    "        s = slice(batch_start, min(num_samples, batch_start + 4))\n",
    "        #feed_dicts.append({\"z\": zs[s], \"y\": ys[s], \"truncation\": truncation, \"step_sizes\": step_sizes[s]})\n",
    "        feed_dicts.append({\"z\": zs[s], \"truncation\": truncation, \"step_sizes\": step_sizes[s]})\n",
    "\n",
    "\n",
    "    for feed_dict in feed_dicts:\n",
    "        ims_batch = []\n",
    "        outscores_batch = []\n",
    "        z_start = feed_dict[\"z\"]\n",
    "\n",
    "        step_sizes = feed_dict[\"step_sizes\"]\n",
    "        \n",
    "        #if opts[\"mode\"] == \"iterative\": \n",
    "        # choose from iterative or bigger_step\n",
    "        mode = \"iterative\"\n",
    "        if mode == \"iterative\":\n",
    "            print(\"iterative\")\n",
    "\n",
    "            # original seed image\n",
    "            #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "            x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=False)\n",
    "\n",
    "            x = np.uint8(x)\n",
    "            if annotate:\n",
    "                #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                #ims_batch.append(common.annotate_outscore(x, outscore))\n",
    "                ims_batch.append(annotate_outscore(x, outscore))\n",
    "\n",
    "\n",
    "            else:\n",
    "                if annotate:\n",
    "                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                    #ims_batch.append(common.annotate_outscore(x, outscore))\n",
    "                    ims_batch.append(annotate_outscore(x, outscore))\n",
    "\n",
    "\n",
    "                else:\n",
    "                    ims_batch.append(x)\n",
    "            outscores_batch.append(outscore)\n",
    "\n",
    "            # negative clone images\n",
    "            z_next = z_start\n",
    "            step_sizes = -step_sizes\n",
    "            for iter in range(0, iters, 1):\n",
    "                feed_dict[\"step_sizes\"] = step_sizes\n",
    "                feed_dict[\"z\"] = z_next\n",
    "                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                x = np.uint8(x)\n",
    "                z_next = tmp\n",
    "                if annotate:\n",
    "                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                    #ims_batch.append(common.annotate_outscore(x, outscore))\n",
    "                    ims_batch.append(annotate_outscore(x, outscore))\n",
    "\n",
    "                else:\n",
    "                    if annotate:\n",
    "                        #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                        ims_batch.append(common.annotate_outscore(x, outscore))\n",
    "\n",
    "                    else:\n",
    "                        ims_batch.append(x)\n",
    "                outscores_batch.append(outscore)\n",
    "\n",
    "            ims_batch.reverse()\n",
    "\n",
    "            # positive clone images\n",
    "            step_sizes = -step_sizes\n",
    "            z_next = z_start\n",
    "            for iter in range(0, iters, 1):\n",
    "                feed_dict[\"step_sizes\"] = step_sizes\n",
    "                feed_dict[\"z\"] = z_next\n",
    "\n",
    "                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                \n",
    "                x = np.uint8(x)\n",
    "                z_next = tmp\n",
    "\n",
    "                if annotate:\n",
    "                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                    ims_batch.append(annotate_outscore(x, outscore))\n",
    "\n",
    "                else:\n",
    "                    ims_batch.append(x)\n",
    "                outscores_batch.append(outscore)\n",
    "\n",
    "        else:\n",
    "            print(\"bigger_step\")\n",
    "\n",
    "            # original seed image\n",
    "            #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=False)\n",
    "            x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=False)\n",
    "            x = np.uint8(x)\n",
    "            if annotate:\n",
    "                ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "            else:\n",
    "                ims_batch.append(x)\n",
    "            outscores_batch.append(outscore)\n",
    "\n",
    "            # negative clone images\n",
    "            step_sizes = -step_sizes\n",
    "            for iter in range(0, iters, 1):\n",
    "                feed_dict[\"step_sizes\"] = step_sizes * (iter + 1)\n",
    "\n",
    "                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "\n",
    "                x = np.uint8(x)\n",
    "\n",
    "                if annotate:\n",
    "                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                    ims_batch.append(annotate_outscore(x, outscore))\n",
    "                else:\n",
    "                    ims_batch.append(x)\n",
    "                outscores_batch.append(outscore)\n",
    "\n",
    "            ims_batch.reverse()\n",
    "            outscores_batch.reverse()\n",
    "\n",
    "            # positive clone images\n",
    "            step_sizes = -step_sizes\n",
    "            for iter in range(0, iters, 1):\n",
    "                feed_dict[\"step_sizes\"] = step_sizes * (iter + 1)\n",
    "\n",
    "                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n",
    "                x = np.uint8(x)\n",
    "                if annotate:\n",
    "                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
    "                    ims_batch.append(annotate_outscore(x, outscore))\n",
    "\n",
    "                else:\n",
    "                    ims_batch.append(x)\n",
    "                outscores_batch.append(outscore)\n",
    "\n",
    "        ims_batch = [np.expand_dims(im, 0) for im in ims_batch]\n",
    "        ims_batch = np.concatenate(ims_batch, axis=0)\n",
    "        ims_batch = np.transpose(ims_batch, (1, 0, 2, 3, 4))\n",
    "        ims.append(ims_batch)\n",
    "\n",
    "        outscores_batch = [np.expand_dims(outscore, 0) for outscore in outscores_batch]\n",
    "        outscores_batch = np.concatenate(outscores_batch, axis=0)\n",
    "        outscores_batch = np.transpose(outscores_batch, (1, 0, 2))\n",
    "        outscores.append(outscores_batch)\n",
    "\n",
    "    ims = np.concatenate(ims, axis=0)\n",
    "    outscores = np.concatenate(outscores, axis=0)\n",
    "    ims_final = np.reshape(ims, (ims.shape[0] * ims.shape[1], ims.shape[2], ims.shape[3], ims.shape[4]))\n",
    "    #I = PIL.Image.fromarray(utils.common.imgrid(ims_final, cols=iters * 2 + 1))\n",
    "    I = PIL.Image.fromarray(common.imgrid(ims_final, cols=iters * 2 + 1))\n",
    "    \n",
    "    #TODO change the code below to write a new file for every result cycle. Currently it over writes the my_results.jpg file. \n",
    "    # For now, make sure to download your result \"my_results.jpg\" before running again\n",
    "    result_dir = \"my_results\"\n",
    "    I.save(os.path.join(result_dir + \".jpg\"))\n",
    "\n",
    "    #I.save(os.path.join(result_dir, categories[y] + \".jpg\"))\n",
    "    print(\"y: \", y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
